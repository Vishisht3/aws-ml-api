# aws-ml-api

This repository contains a production-inspired Machine Learning API deployed on AWS. It performs real-time sentiment analysis by accepting text input and returning sentiment predictions.

The API is built using:

- **Amazon SageMaker Autopilot** for automated model training and deployment
- **AWS API Gateway** configured with JWT authorization and rate limiting for secure API exposure
- **AWS Lambda** functions handling request validation and invoking SageMaker endpoints
- **CloudWatch** dashboards and alarms monitoring latency, error rates, and service health
- Canary deployment strategy using Lambda aliases to safely roll out new model versions
- Input validation with JSON schemas to ensure well-formed requests
- IAM roles with least-privilege access policies restricting Lambda and SageMaker permissions
- Bias detection and model explainability leveraging SageMaker Clarify jobs

---

## API Overview

- **Endpoint:** `<INSERT_API_ENDPOINT_URL_HERE>`  
- **Input:** JSON object with the following shape:  
{
"text": "I love this product!"
}
- **Output:** JSON object with predicted sentiment label, e.g.:  
{
"sentiment": "positive"
}

---

## Performance Targets

- P95 latency target: 700 milliseconds  
- Availability target: 99.9%  
- Cost goal: Under â‚¹0.50 per 1000 requests (based on actual AWS billing data)

---

## Features

- **Security:** JWT authentication enforced at API Gateway to protect access  
- **Rate Limiting:** Configured burst and steady-state limits to prevent abuse  
- **Versioning:** Canary releases with traffic split controlled via Lambda aliases  
- **Observability:** CloudWatch dashboards track invocations, latency percentiles, and error rates  
- **Rollback:** Automated rollback policies triggered by SLA breaches or elevated error counts  
- **Governance:** Bias and feature attribution reports generated through SageMaker Clarify  
- **Input Validation:** Requests validated against JSON schema, oversized payloads rejected with clear error messages

---

## Getting Started

1. Clone this repository  
2. See detailed **Architecture and Specification** in `/docs/spec.md`  
3. Follow deployment instructions in `/docs/deployment.md` to provision the AWS stack  
4. Use `/docs/postman.json` collection or curl commands in `/docs/demo.md` to test API  
5. Refer to `/docs/runbook.md` for monitoring and incident response procedures  

---

## Limitations & Future Work

- Current model used: SageMaker Autopilot best model on Amazon Customer Reviews dataset  
- Future plans: Add batch inference and caching for cost and latency optimization  
- Additional planned improvements include tighter IAM policies and advanced alerting  

---

## License

This project is licensed under the MIT License.

---

If you find this project useful or have questions, please open an issue or make a pull request.
